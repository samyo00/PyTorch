{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Backpropagation ***\n",
        "\n",
        "3 steps\n",
        "\n",
        "1) Forward pass : Compute Loss\n",
        "\n",
        "2) Compute Local Gradients \n",
        "\n",
        "3) Backward pass : Compute dLoss/dWeights using the chain rule"
      ],
      "metadata": {
        "id": "0eUyfMv_s_F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Now Let's see how can we use it on pytorch***"
      ],
      "metadata": {
        "id": "mtyfO2iYvgP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xksYuTgUdjTV"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0)\n",
        "\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "fY9qHUABdmIN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Forward pass and compute the loss***"
      ],
      "metadata": {
        "id": "wBWlgTvVv8IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = w*x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ewe2SGNvrDM",
        "outputId": "08893dbc-feec-4ac2-98eb-76a1ce4453d8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Backwardpass***"
      ],
      "metadata": {
        "id": "QFf3s20hwK9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch will calculate local gradients for us and will also computes the backward pass automatically\n",
        "\n",
        "loss.backward() # This is the whole gradient calculation \n",
        "print(w.grad) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2ToLF1kwI5B",
        "outputId": "a7ec7fa6-7c9e-472f-bde1-0694dfd44de8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In Step 1 :***\n",
        "\n",
        "1.   Prediction : Manually\n",
        "2.   Gradient Computation : Manually\n",
        "3.   Loss Computation : Manually\n",
        "4.   Parameter Updates : Manually\n",
        "\n",
        "***In Step 2 :***\n",
        "\n",
        "1.   Prediction : Manually\n",
        "2.   Gradient Computation : Autograd\n",
        "3.   Loss Computation : Manually\n",
        "4.   Parameter Updates : Manually\n",
        "\n",
        "***In Step 3 :***\n",
        "\n",
        "1.   Prediction : Manually\n",
        "2.   Gradient Computation : Autograd\n",
        "3.   Loss Computation : PyTorch Loss\n",
        "4.   Parameter Updates : PyTorch Optimizer\n",
        "\n",
        "\n",
        "***In Step 4 :***\n",
        "\n",
        "1.   Prediction : PyTorch Model\n",
        "2.   Gradient Computation : Autograd\n",
        "3.   Loss Computation : PyTorch Loss\n",
        "4.   Parameter Updates : PyTorch Optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tpm8WTvLwzt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "\n",
        "# f = 2 * x\n",
        "\n",
        "# some training examples \n",
        "\n",
        "x = np.array([1,2,3,4], dtype=np.float32)\n",
        "y = np.array([2,4,6,8], dtype=np.float32)\n",
        "\n",
        "# Initializing our weights \n",
        "\n",
        "w = 0.0\n",
        "\n",
        "# model prediction \n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "\n",
        "# Loss = MSE\n",
        "\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE = 1/N * (w*x-y)**2\n",
        "# dJ/dw = 1/N 2x (w*x-y)\n",
        "\n",
        "def gradient(x,y,y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training \n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "  # prediction = forward pass \n",
        "  y_pred = forward(x)\n",
        "\n",
        "  # loss\n",
        "  l = loss(y,y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(x,y,y_pred)\n",
        "\n",
        "  # update weights \n",
        "  w -= learning_rate * dw\n",
        " \n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch {epoch+1}: w={w:.3f},loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqnIW_9mwzW3",
        "outputId": "60ec07ba-8a4d-4fa9-8b42-7033ae00d04a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training : f(5) = 0.000\n",
            "epoch 1: w=1.200,loss = 30.00000000\n",
            "epoch 3: w=1.872,loss = 0.76800019\n",
            "epoch 5: w=1.980,loss = 0.01966083\n",
            "epoch 7: w=1.997,loss = 0.00050331\n",
            "epoch 9: w=1.999,loss = 0.00001288\n",
            "epoch 11: w=2.000,loss = 0.00000033\n",
            "epoch 13: w=2.000,loss = 0.00000001\n",
            "epoch 15: w=2.000,loss = 0.00000000\n",
            "epoch 17: w=2.000,loss = 0.00000000\n",
            "epoch 19: w=2.000,loss = 0.00000000\n",
            "Prediction after training : f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Everything with pyTorch and get rid of gradient(Manually)***"
      ],
      "metadata": {
        "id": "BNcQhvJO7ZB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "\n",
        "# f = 2 * x\n",
        "\n",
        "# some training examples \n",
        "\n",
        "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "# Initializing our weights \n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32,requires_grad=True)\n",
        "\n",
        "# model prediction \n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "\n",
        "# Loss = MSE\n",
        "\n",
        "def loss(y,y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE = 1/N * (w*x-y)**2\n",
        "# dJ/dw = 1/N 2x (w*x-y)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training \n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "  # prediction = forward pass \n",
        "  y_pred = forward(x)\n",
        "\n",
        "  # loss\n",
        "  l = loss(y,y_pred)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update weights \n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # Zero gradients\n",
        "  w.grad.zero_()\n",
        " \n",
        "  if epoch % 10 == 0:\n",
        "    print(f'epoch {epoch+1}: w={w:.3f},loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgojGD5Lwukp",
        "outputId": "b16b7aad-2d37-4c1b-a08b-5652bbf18872"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training : f(5) = 0.000\n",
            "epoch 1: w=0.300,loss = 30.00000000\n",
            "epoch 11: w=1.665,loss = 1.16278565\n",
            "epoch 21: w=1.934,loss = 0.04506890\n",
            "epoch 31: w=1.987,loss = 0.00174685\n",
            "epoch 41: w=1.997,loss = 0.00006770\n",
            "epoch 51: w=1.999,loss = 0.00000262\n",
            "epoch 61: w=2.000,loss = 0.00000010\n",
            "epoch 71: w=2.000,loss = 0.00000000\n",
            "epoch 81: w=2.000,loss = 0.00000000\n",
            "epoch 91: w=2.000,loss = 0.00000000\n",
            "Prediction after training : f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Setp 3 & 4***"
      ],
      "metadata": {
        "id": "z8WTOzPT-SOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Pipeline: Model, Loss, and Optimizer"
      ],
      "metadata": {
        "id": "YBVldQ6q-aoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***General training pipleine in pyTorch:***\n",
        "\n",
        "3 steps :\n",
        "\n",
        "1.   Design Model (input,output size,forward pass)\n",
        "\n",
        "2.   Construct the loss and optimizer\n",
        "3.   Training loop\n",
        "       -forward pass: compute prediction\n",
        "       -backward pass : gradients\n",
        "       -update weights \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1VgFluj-ns0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "\n",
        "# f = 2 * x\n",
        "\n",
        "# some training examples \n",
        "\n",
        "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
        "\n",
        "# Initializing our weights \n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32,requires_grad=True)\n",
        "\n",
        "# model prediction \n",
        "\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training \n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD([w],lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "  # prediction = forward pass \n",
        "  y_pred = forward(x)\n",
        "\n",
        "  # loss\n",
        "  l = loss(y,y_pred)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update weights \n",
        "  optimizer.step()\n",
        "\n",
        "  # Zero gradients\n",
        "  optimizer.zero_grad()\n",
        " \n",
        "  if epoch % 10 == 0:\n",
        "    print(f'epoch {epoch+1}: w={w:.3f},loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVD-ycws8ajk",
        "outputId": "39e2b9b9-b4e3-4e43-d6a3-4ef29608741d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training : f(5) = 0.000\n",
            "epoch 1: w=0.300,loss = 30.00000000\n",
            "epoch 11: w=1.665,loss = 1.16278565\n",
            "epoch 21: w=1.934,loss = 0.04506890\n",
            "epoch 31: w=1.987,loss = 0.00174685\n",
            "epoch 41: w=1.997,loss = 0.00006770\n",
            "epoch 51: w=1.999,loss = 0.00000262\n",
            "epoch 61: w=2.000,loss = 0.00000010\n",
            "epoch 71: w=2.000,loss = 0.00000000\n",
            "epoch 81: w=2.000,loss = 0.00000000\n",
            "epoch 91: w=2.000,loss = 0.00000000\n",
            "Prediction after training : f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 4***"
      ],
      "metadata": {
        "id": "yLEpd9aOCz6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f = w * x\n",
        "\n",
        "# f = 2 * x\n",
        "\n",
        "# some training examples \n",
        "\n",
        "# this must be 2D array now\n",
        "\n",
        "x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32) \n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor([5],dtype=torch.float32)\n",
        "\n",
        "\n",
        "n_samples, n_features= x.shape\n",
        "print(n_samples,n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(input_size,output_size)\n",
        "\n",
        "# Custom model\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "    #define our layers\n",
        "    self.lin = nn.Linear(input_dim,output_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model=LinearRegression(input_size,output_size)\n",
        "\n",
        "\n",
        "print(f'Prediction before training : f(5) = {model(x_test).item():.3f}')\n",
        "\n",
        "# Training \n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "  # prediction = forward pass \n",
        "  y_pred = model(x)\n",
        "\n",
        "  # loss\n",
        "  l = loss(y,y_pred)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update weights \n",
        "  optimizer.step()\n",
        "\n",
        "  # Zero gradients\n",
        "  optimizer.zero_grad()\n",
        " \n",
        "  if epoch % 10 == 0:\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w={w[0][0].item():.3f},loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction before training : f(5) = {model(x_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh5iI0SbCr9H",
        "outputId": "ecdaab92-1a80-4498-f797-25e4144d4abe"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training : f(5) = -1.293\n",
            "epoch 1: w=0.237,loss = 41.62461090\n",
            "epoch 11: w=1.714,loss = 1.07693768\n",
            "epoch 21: w=1.952,loss = 0.02787174\n",
            "epoch 31: w=1.990,loss = 0.00072936\n",
            "epoch 41: w=1.996,loss = 0.00002662\n",
            "epoch 51: w=1.998,loss = 0.00000799\n",
            "epoch 61: w=1.998,loss = 0.00000709\n",
            "epoch 71: w=1.998,loss = 0.00000666\n",
            "epoch 81: w=1.998,loss = 0.00000627\n",
            "epoch 91: w=1.998,loss = 0.00000591\n",
            "Prediction before training : f(5) = 9.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIFyTKFkDypk"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}